#!/bin/bash
#SBATCH -J job
#SBATCH --account=camp-kp
#SBATCH --partition=camp-kp
#SBATCH --time=72:00:00
#SBATCH --nodes 1
#SBATCH --ntasks 10
#SBATCH -o job.out # stdout
#SBATCH -e job.err # stderr

ulimit -u 10000 #optional, setting user limits. The resources from jobs can
#add up quickly. Likely not a problem at chpc though. 

parallel="parallel --delay .2 -j $SLURM_NTASKS" 
#$SLURM_NTASKS is how many cores the slurm script can use. It is good
#to set this using prior knowledge based on the parition you are using
#--delay .2 prevents overloading the controlling node

#Create a function that does whatever it is you want to do
foo () {
    i=$1; #Unnecessary, but I prefer to use i
    echo "$i"; #Do something with or without i. 
    push Job $i has finished. #This is going to be unruly if you are running
    #a lot of things. 
}

#Export your function for use. 
export -f foo  

#pass a set of values to your function wrapped in parallel. I typically use
#a sequence of integers, but you can iterate over whatever you want. I also
#typically use the intergers as indicies for an array that may be more 
#complicated to pass to parallel
seq $first $last | $parallel foo

#the sequence is labelled as first and last, because you could create 
#another basgh script that iterates over 